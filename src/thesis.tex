\documentclass[12pt,a4paper,twoside,openany]{book}

%% build: latexmk -pdf -pvc thesis.tex

%% Packages
%% --------------------------------------------------------------------------------

\usepackage{geometry}
\usepackage[utf8]{inputenc}
\usepackage[hidelinks]{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathpartir}
\usepackage{scalerel}
\usepackage{stmaryrd}
\usepackage{authblk}
\usepackage{todonotes}
\presetkeys{todonotes}{inline}{}
\usepackage{bm}
\usepackage{titlesec}

%% \bibliographystyle{IEEEtran}
\bibliographystyle{alpha}

%% Environments
%% --------------------------------------------------------------------------------

\theoremstyle{remark}
\newtheorem{notation}{Notation}

\theoremstyle{definition}
\newtheorem{mydefinition}{Definition}
\newtheorem{myexample}{Example}
\newtheorem{mylemma}{Lemma}

%% Fonts and spacing
%% --------------------------------------------------------------------------------
\linespread{1.25}
%% \geometry{left=3cm,right=2cm,top=2.5cm,bottom=2.5cm,includehead,includefoot}
\geometry{left=4cm,right=2.5cm,top=2.5cm,bottom=2.5cm,includehead,includefoot}


%% Chapter style
%% --------------------------------------------------------------------------------

\titleformat{\chapter}[display]
  {\bfseries\Large}
  {\filright\MakeUppercase{\chaptertitlename} \Huge\thechapter}
  {1ex}
  {\titlerule\vspace{1ex}\filleft}
  [\vspace{1ex}\titlerule]


%% Abbrevs
%% --------------------------------------------------------------------------------

\newcommand{\mi}[1]{\mathit{#1}}
\newcommand{\ms}[1]{\mathsf{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\mbf}[1]{\mathbf{#1}}

\newcommand{\refl}{\mathsf{refl}}
\newcommand{\reflect}{\mathsf{reflect}}
\newcommand{\id}{\mathsf{id}}
\newcommand{\Con}{\mathsf{Con}}
\newcommand{\Sub}{\mathsf{Sub}}
\newcommand{\Tm}{\mathsf{Tm}}
\newcommand{\Ty}{\mathsf{Ty}}
\newcommand{\U}{\mathsf{U}}
\newcommand{\El}{\mathsf{El}}
\newcommand{\Id}{\mathsf{Id}}
\newcommand{\proj}{\mathsf{proj}}
\renewcommand{\tt}{\mathsf{tt}}
\newcommand{\blank}{\mathord{\hspace{1pt}\text{--}\hspace{1pt}}}
\newcommand{\ra}{\rightarrow}
\newcommand{\Set}{\mathsf{Set}}
\newcommand{\Lift}{\Uparrow}
\newcommand{\ToS}{\mathsf{ToS}}
\newcommand{\ext}{\triangleright}
\newcommand{\emptycon}{\scaleobj{.75}\bullet}
\newcommand{\Pii}{\Pi}
\newcommand{\appi}{\mathsf{app}}
\newcommand{\lami}{\mathsf{lam}}
\newcommand{\Pie}{\Pi^{\mathsf{ext}}}
\newcommand{\appe}{\mathsf{app^{ext}}}
\newcommand{\lame}{\mathsf{lam^{ext}}}
\newcommand{\Piinf}{\Pi^{\mathsf{inf}}}
\newcommand{\appinf}{\mathsf{app^{inf}}}
\newcommand{\laminf}{\mathsf{lam^{inf}}}
\newcommand{\appitt}{\mathop{{\scriptstyle @}}}
\newcommand{\Refl}{\mathsf{Refl}}
\newcommand{\IdU}{\mathsf{IdU}}
\newcommand{\ReflU}{\mathsf{ReflU}}
\newcommand{\Sig}{\mathsf{Sig}}
\newcommand{\ToSSig}{\mathsf{ToSSig}}
\newcommand{\Subtype}{\mathsf{Subtype}}
\newcommand{\subtype}{\mathsf{subtype}}
\newcommand{\NatSig}{\mathsf{NatSig}}
\newcommand{\Sg}{\Sigma}
\newcommand{\flCwF}{\mathsf{flCwF}}
\newcommand{\Kfam}{\mathsf{K}}
\newcommand{\p}{\mathsf{p}}
\newcommand{\q}{\mathsf{q}}
\newcommand{\K}{\mathsf{K}}
\newcommand{\lamK}{\mathsf{lam}^{\K}}
\newcommand{\appK}{\mathsf{app}^{\K}}
\newcommand{\A}{\mathsf{A}}
\newcommand{\D}{\mathsf{D}}
\renewcommand{\S}{\mathsf{S}}
\newcommand{\arri}{\Rightarrow}
\newcommand{\arre}{\Rightarrow^{\mathsf{ext}}}
\newcommand{\arrinf}{\Rightarrow^{\mathsf{inf}}}
\newcommand{\syn}{\mathsf{syn}}
\newcommand{\SynSig}{\mathsf{SynSig}}
\newcommand{\bCon}{\boldsymbol{\Con}}
\newcommand{\bTy}{\boldsymbol{\Ty}}
\newcommand{\bSub}{\boldsymbol{\Sub}}
\newcommand{\bTm}{\boldsymbol{\Tm}}
\newcommand{\bGamma}{\boldsymbol{\Gamma}}
\newcommand{\bDelta}{\boldsymbol{\Delta}}
\newcommand{\bsigma}{\boldsymbol{\sigma}}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\bepsilon}{\boldsymbol{\epsilon}}
\newcommand{\bt}{\boldsymbol{t}}
\newcommand{\bu}{\boldsymbol{u}}
\newcommand{\bA}{\boldsymbol{A}}
\newcommand{\ba}{\boldsymbol{a}}
\newcommand{\bb}{\boldsymbol{b}}
\newcommand{\bB}{\boldsymbol{B}}
\newcommand{\bid}{\boldsymbol{\id}}
\newcommand{\bemptycon}{\boldsymbol{\emptycon}}
\newcommand{\bSet}{\boldsymbol{\Set}}
\newcommand{\bU}{\boldsymbol{\U}}
\newcommand{\bEl}{\boldsymbol{\El}}
\newcommand{\bPii}{\boldsymbol{\Pi}}
\newcommand{\bPie}{\boldsymbol{\Pie}}
\newcommand{\bPiinf}{\boldsymbol{\Piinf}}
\newcommand{\bappi}{\boldsymbol{\mathsf{app}}}
\newcommand{\blami}{\boldsymbol{\mathsf{lam}}}
\newcommand{\bId}{\boldsymbol{\Id}}
\newcommand{\bM}{\boldsymbol{\mathsf{M}}}
\newcommand{\bT}{\boldsymbol{\mathsf{T}}}
\newcommand{\bS}{\boldsymbol{\mathsf{S}}}
\newcommand{\bP}{\boldsymbol{\mathsf{P}}}
\newcommand{\bD}{\boldsymbol{\mathsf{D}}}
\newcommand{\bI}{\boldsymbol{\mathsf{I}}}
\newcommand{\ul}[1]{\underline{#1}}
\newcommand{\ulGamma}{\ul{\Gamma}}
\newcommand{\ulDelta}{\ul{\Delta}}
\newcommand{\ulgamma}{\ul{\gamma}}
\newcommand{\ulOmega}{\ul{\Omega}}
\newcommand{\uldelta}{\ul{\delta}}
\newcommand{\ulsigma}{\ul{\sigma}}
\newcommand{\ulnu}{\ul{\nu}}
\newcommand{\ulepsilon}{\ul{\epsilon}}
\newcommand{\ult}{\ul{t}}
\newcommand{\ulu}{\ul{u}}
\newcommand{\ulA}{\ul{A}}
\newcommand{\ula}{\ul{a}}
\newcommand{\ulB}{\ul{B}}
\newcommand{\tos}{\mathsf{tos}}
\newcommand{\coe}{\mathsf{coe}}
\newcommand{\coh}{\mathsf{coh}}
\newcommand{\llb}{\llbracket}
\newcommand{\rrb}{\rrbracket}

\newcommand{\Var}{\ms{Var}}
\newcommand{\var}{\ms{var}}
\newcommand{\app}{\ms{app}}
\newcommand{\vz}{\ms{vz}}
\newcommand{\vs}{\ms{vs}}
\newcommand{\Alg}{\ms{Alg}}
\newcommand{\Mor}{\ms{Mor}}
\newcommand{\DispAlg}{\ms{DispAlg}}
\newcommand{\Section}{\ms{Section}}
\newcommand{\Initial}{\ms{Initial}}
\newcommand{\Inductive}{\ms{Inductive}}
\newcommand{\TmAlg}{\ms{TmAlg}}
\newcommand{\Rec}{\ms{Rec}}
\newcommand{\Ind}{\ms{Ind}}
\newcommand{\Obj}{\ms{Obj}}
\newcommand{\Nat}{\ms{Nat}}
\newcommand{\Bool}{\ms{Bool}}
\newcommand{\mbbC}{\mbb{C}}
\newcommand{\mbbD}{\mbb{D}}
\newcommand{\lam}{\ms{lam}}

\newcommand{\true}{\ms{true}}
\newcommand{\false}{\ms{false}}



%% --------------------------------------------------------------------------------

\title{Type-Theoretic Signatures for Inductive Types}
\date{2021 September}
\author{András Kovács}

%% --------------------------------------------------------------------------------

\begin{document}
\maketitle

\frontmatter
\tableofcontents{}


\mainmatter

\chapter{Introduction}

\section{Specification and Semantics for Inductive Types}
\section{Overview of the Thesis and Contributions}
\section{Notation and Conventions}
\subsection{Metatheory}
\subsection{Universes}
\label{sec:notation}

\chapter{Simple Inductive Signatures}
\label{chap:simple-inductive-signatures}

In this chapter, we take a look at a very simple notion of inductive
signature. The motivation for doing so is to present the basic ideas of this
thesis in the easiest possible setting, with explicit definitions. The later
chapters are greatly generalized and expanded compared to the current one, and
are not feasible (and probably not that useful) to present in full formal
detail. We also include a complete Agda formalization of the contents of this
chapter, in less than 200 lines.

\todo{potentially in intro}

The mantra throughout this dissertation is the following: inductive types are
specified by typing contexts in certain \emph{theories of signatures}. For each
class of inductive types, there is a corresponding theory of signatures, which
is viewed as a proper type theory and comes equipped with an algebraic model
theory. \emph{Semantics} of signatures is given by interpreting them in certain
models of the theory of signatures. Semantics should at least provide a notion
of induction principle for each signature; in this chapter we provide a bit more
than that, and substantially more in Chapters \ref{chap:fqiit} and \ref{chap:iqiit}.

\section{Theory of Signatures}
\label{sec:simple-signatures}

Generally, more expressive theories of signatures can describe a larger classes
of inductive types. As we are aiming at minimalism right now, the current theory
of signatures is as follows:

\begin{mydefinition}The \emph{theory of signatures}, or ToS for short in the current chapter,
is a simple type theory equipped with the following features:
  \begin{itemize}
    \item An empty base type $\iota$.
    \item A \emph{first-order function type} $\iota\!\to\!\blank$; this is a
      function whose domain is fixed to be $\iota$. Moreover, first-order functions only
      have neutral terms: there is application, but no $\lambda$-abstraction.
  \end{itemize}
\end{mydefinition}

We can specify the full syntax using the following Agda-like inductive definitions.
\begin{alignat*}{4}
  & \Ty              &&: \Set           && \Var &&: \Con \to \Ty \to \Set \\
  & \iota            &&: \Ty            && \vz  &&: \Var\,(\Gamma \ext A)\,A \\
  & \iota\!\to\blank &&: \Ty \to \Ty    && \vs  &&: \Var\,\Gamma\,A \to \Var\,(\Gamma \ext B)\,A\\
  & && && &&\\
  & \Con             &&: \Set           && \Tm  &&: \Con \to \Ty \to \Set \\
  & \emptycon        &&: \Con           && \var &&: \Var\,\Gamma\,A \to \Tm\,\Gamma\,A \\
  & \blank\ext\blank &&: \Con \to \Ty \to \Con \hspace{2em} && \app &&: \Tm\,\Gamma\,(\iota\to A) \to \Tm\,\Gamma\,\iota
                                                           \to \Tm\,\Gamma\,A
\end{alignat*}
Here, $\Con$ contexts are lists of types, and $\Var$ specifies well-typed De Bruijn indices, where
$\vz$ represents the zero index, and $\vs$ takes the successor of an index.

\begin{notation} We use capital Greek letters starting from $\Gamma$ to refer to contexts, $A$, $B$, $C$ to
refer to types, and $t$, $u$, $v$ to refer to terms. In examples, we may use a
nameful notation instead of De Bruijn indices. For example, we may write $x :
\Tm\,(\emptycon \ext (x : \iota) \ext (y : \iota))\,\iota$ instead of $\var\,(\vs\,\vz)
: \Tm\,(\emptycon \ext \iota \ext \iota)\,\iota$. Additionally, we may write
$t\,u$ instead of $\app\,t\,u$ for $t$ and $u$ terms.
\end{notation}

\begin{mydefinition} \emph{Parallel substitutions} map variables to terms.
\begin{alignat*}{3}
&\Sub : \Con \to \Con \to \Set\\
&\Sub\,\Gamma\,\Delta \equiv \{A\} \to \Var\,\Delta\,A \to \Tm\,\Gamma\,A
\end{alignat*}
We use $\sigma$ and $\delta$ to refer to substitutions. We also recursively
define the action of substitution on terms:
\begin{alignat*}{3}
  &\rlap{$\blank[\blank] : \Tm\,\Delta\,A \to \Sub\,\Gamma\,\Delta \to \Tm\,\Gamma\,A$}\\
  &(\var\, x)   &&[ \sigma ] \equiv \sigma\,x\\
  &(\app\,t\,u) &&[ \sigma ] \equiv \app\,(t[\sigma])\,(u[\sigma])
\end{alignat*}
The identity substitution $\id$ is defined simply as $\var$. It is easy to see that
$t[\id] = t$ for all $t$. Substitution composition is as follows.
\begin{alignat*}{3}
  &\blank\!\circ\!\blank : \Sub\,\Delta\,\Xi \to \Sub\,\Gamma\,\Delta \to \Sub\,\Gamma\,\Xi\\
  &(\sigma \circ \delta)\,x \equiv (\sigma\,x)[\delta]
\end{alignat*}
\end{mydefinition}

\begin{myexample} We may write signatures for natural numbers and binary trees respectively as follows.
\begin{alignat*}{3}
  & \ms{NatSig}  &&\equiv \emptycon \ext (\mi{zero} : \iota) \ext (\mi{suc} : \iota \to \iota)\\
  & \ms{TreeSig} &&\equiv \emptycon \ext (\mi{leaf} : \iota) \ext (\mi{node} : \iota \to \iota \to \iota)
\end{alignat*}
\end{myexample}

In short, the current ToS allows inductive types which are
\begin{itemize}
\item \emph{Single-sorted}: this means that we have a single type constructior, corresponding to $\iota$.
\item \emph{Closed}: signatures cannot refer to any externally existing type. For example, we cannot write a signature for ``lists of natural number'' in a direct fashion, since there is no way to refer to the type of natural numbers.
\item \emph{Finitary}: inductive types corresponding to signatures are always
  finitely branching trees. Being closed implies being finitary, since an
  infinitely branching node would require some external type to index subtrees
  with. For example, $\mi{node} : (\mathbb{N} \to \iota) \to \iota$ would
  specify an infinite branching (if such type was allowed in ToS).
\end{itemize}

\emph{Remark.} We omit $\lambda$-expressions from ToS for the sake of
simplicity: this causes terms to be always in normal form (neutral, to be
precise), and thus we can skip talking about conversion rules. Later, starting
from Chapter \ref{chap:fqiit} we include proper $\beta\eta$-rules in signature
theories.

\section{Semantics}
\label{sec:simple-semantics}

For each signature, we need to know what it means for a type theory to support
the corresponding inductive type. For this, we need at least a notion of
\emph{algebras}, which can be viewed as a bundle of all type and
value constructors, and what it means for an algebra to support an
\emph{induction principle}.  Additionally, we may want to know what it means to
support a \emph{recursion principle}, which can be viewed as a non-dependent
variant of induction. In the following, we define these notions by induction on
ToS syntax.

\emph{Remark.} We use the terms ``algebra'' and ``model'' synonymously throughout
this thesis.

\subsection{Algebras}

First, we calculate types of algebras. This is simply a standard interpretation
into the $\Set$ universe. We define the following operations by induction; the
$\blank^A$ name is overloaded for $\Con$, $\Ty$ and $\Tm$.
\begin{alignat*}{3}
& \hspace{-4em} \rlap{$\blank^A : \Ty \to \Set \to \Set$} \\
& \hspace{-4em} \iota^A\,&&X \equiv X \\
& \hspace{-4em} (\iota\to A)^A\,&&X \equiv X \to A^A\,X\\
& \hspace{-4em} && \\
& \hspace{-4em} \rlap{$\blank^A : \Con \to \Set \to \Set$}\\
& \hspace{-4em} \rlap{$\Gamma^A\,X \equiv \{A : \Ty\} \to \Var\,\Gamma\,A \to A^A\,X$}\\
& \hspace{-4em} && \\
& \hspace{-4em} \rlap{$\blank^A : \Tm\,\Gamma\,A \to \{X : \Set\} \to \Gamma^A\,X \to A^A\,X$}\\
& \hspace{-4em} (\var\,x)^A\,&&\gamma \equiv \gamma\,x\\
& \hspace{-4em} (\app\,t\,u)^A\,&&\gamma \equiv t^A\,\gamma\,(u^A\,\gamma)\\
& \hspace{-4em} && \\
& \hspace{-4em} \rlap{$\blank^A : \Sub\,\Gamma\,\Delta \to \{X : \Set\} \to \Gamma^A\,X \to \Delta^A\,X$}\\
& \hspace{-4em} \rlap{$\sigma^A\,\gamma\,x \equiv (\sigma\,x)^A\,\gamma$}
\end{alignat*}
Here, types and contexts depend on some $X : \Set$, which serves as the
interpretation of $\iota$. We define $\Gamma^A$ as a product: for each variable
in the context, we get a semantic type. This trick, along with the definition of
$\Sub$, makes formalization a bit more compact. Terms and substitutions are
interpreted as natural maps. Substitutions are interpreted by pointwise interpreting
the contained terms.

\begin{notation}
We may write values of $\Gamma^A$ using notation for $\Sigma$-types. For
example, we may write $(\mi{zero} : X) \times (\mi{suc} : X \to X)$ for the
result of computing $\ms{NatSig}^A\,X$.
\end{notation}

\begin{mydefinition} We define \emph{algebras} as follows.
\begin{alignat*}{3}
  & \Alg : \Con \to \Set_1 \\
  & \Alg\,\Gamma \equiv (X : \Set) \times \Gamma^A\,X
\end{alignat*}
\end{mydefinition}

\begin{myexample} $\Alg\,\ms{NatSig}$ is computed to $(X : \Set)\times(\mi{zero} :
X)\times(\mi{suc} : X \to X)$.
\end{myexample}

\subsection{Morphisms}

Now, we compute notions of morphisms of algebras. In this case, morphisms are
functions between underlying sets which preserve all specified structure. The
interpretation for calculating morphisms is a \emph{logical relation
interpretation} \cite{udayReynolds} over the $\blank^A$ interpretation. The key
part is the interpretation of types:
\begin{alignat*}{3}
  & \hspace{-4em}\rlap{$\blank^M : (A : \Ty)\{X_0\,X_1 : \Set\}(X^M : X_0 \to X_1) \to A^A\,X_0 \to A^A\,X_1 \to \Set$}\\
  & \hspace{-4em}\iota^M\,&&X^M\,\alpha_0\,\,\alpha_1 \equiv X^M\,\alpha_0 = \alpha_1 \\
  & \hspace{-4em}(\iota\to A)^M\,&&X^M\,\alpha_0\,\,\alpha_1 \equiv
       (x : X_0) \to A^M\,X^M\,(\alpha_0\,x)\,(\alpha_1\,(X^M\,x))
\end{alignat*}
We again assume an interpretation for the base type $\iota$, as $X_0$, $X_1$ and
$X^M : X_0 \to X_1$. $X^M$ is function between underlying sets of algebras, and
$A^M$ computes what it means that $X^M$ preserves an operation with type $A$. At
the base type, preservation is simply equality. At the first-order function
type, preservation is a quantified statement over $X_0$. We define morphisms for
$\Con$ pointwise:
\begin{alignat*}{3}
  &\Con^M : (\Gamma : \Con)\{X_0\,X_1 : \Set\} \to (X_0 \to X_1) \to \Gamma^A\,X_0 \to \Gamma^A\,X_1 \to \Set\\
  &\Gamma^M\,X^M\,\gamma_0\,\gamma_1 \equiv
    \{A : \Ty\}(x : \Var\,\Gamma\,A) \to A^M\,X^M\,(\gamma_0\,x)\,(\gamma_1\,x)
\end{alignat*}
For terms and substitutions, we get preservation statements, which are sometimes
called \emph{fundamental lemmas} in discussions of logical relations \cite{udayReynolds}.
\begin{alignat*}{3}
  & \hspace{-10em}\rlap{$\blank^M : (t : \Tm\,\Gamma\,A) \to \Gamma^M\,X^M\,\gamma_0\,\gamma_1 \to A^M\,X^M\,(t^A\,\gamma_0)\,(t^A\,\gamma_1)$}\\
  & \hspace{-10em}(\var\,x)^M    &&\gamma^M \equiv \gamma^M\,x \\
  & \hspace{-10em}(\app\,t\,u)^M &&\gamma^M \equiv t^M\,\gamma^M\,(u^A\,\gamma_0)\\
  & \hspace{-10em}&& \\
  & \hspace{-10em}\rlap{$\blank^M : (\sigma : \Sub\,\Gamma\,\Delta) \to \Gamma^M\,X^M\,\gamma_0\,\gamma_1 \to \Delta^M\,X^M\,(\sigma^A\,\gamma_0)\,(\sigma^A\,\gamma_1)$}\\
  & \hspace{-10em} \rlap{$\sigma^M\, \gamma^M\,x = (\sigma\,x)^M\,\gamma^M$}
\end{alignat*}
The definition of $(\app\,t\,u)^M$ is well-typed by the induction hypothesis
$u^M\,\gamma^M : X^M\,(u^A\,\gamma_0) = u^A\,\gamma_1$.

\begin{mydefinition}
We again pack up $\Gamma^M$ with the interpretation of $\iota$, to get notions
of \emph{algebra morphisms}:
\begin{alignat*}{3}
  & \Mor : \{\Gamma : \Con\} \to \Alg\,\Gamma \to \Alg\,\Gamma \to \Set \\
  & \Mor\,\{\Gamma\}\,(X_0,\,\gamma_0)\,(X_1,\,\gamma_1) \equiv (X^M : X_0 \to X_1) \times \Gamma^M\,X^M\,\gamma_0\,\gamma_1
\end{alignat*}
\end{mydefinition}
\begin{myexample} We have the following computation:
\begin{alignat*}{3}
  & \hspace{-5em}\rlap{$\Mor\,\{\NatSig\}\,(X_0,\,\mi{zero_0},\,\mi{suc_0})\,(X_0,\,\mi{zero_1},\,\mi{suc_1}) \equiv$} \\
           &(X^M : X_0 \to X_1) \\
   \times\,&(X^M\,\mi{zero_0} = \mi{zero_1}) \\
   \times\,&((x : X_0) \to X^M\,(\mi{suc_0}\,x) = \mi{suc_1}\,(X^M\,x))
\end{alignat*}
\end{myexample}

\begin{mydefinition} We state \emph{initiality} as a predicate on algebras:
\begin{alignat*}{3}
  & \Initial : \{\Gamma : \Con\} \to \Alg\,\Gamma \to \Set\\
  & \Initial\,\{\Gamma\}\,\gamma \equiv
    (\gamma' : \Alg\,\Gamma) \to \ms{isContr}\,(\Mor\,\Gamma\,\gamma\,\gamma')
\end{alignat*}
Here $\ms{isContr}$ refers to unique existence \cite[Section 3.11]{hottbook}. If we drop
$\ms{isContr}$ from the definition, we get the notion of weak initiality, which
corresponds to the recursion principle for $\Gamma$. Although we call this
predicate $\Initial$, in this chapter we do not yet show that algebras form a
category. We provide the extended semantics in Chapter \ref{chap:fqiit}. The
computed algebras and morphism there remain the same as in the current chapter.
\end{mydefinition}

\paragraph{Morphisms vs.\ logical relations.}
The $\blank^M$ interpretation can be viewed as a special case of logical
relations over the $\blank^A$ model: every morphism is a \emph{functional}
logical relation, where the chosen relation between the underlying sets happens
to be a function. Consider now a more general relational interpretation for
types:
\begin{alignat*}{3}
  & \hspace{-0.5em}\rlap{$\blank^R : (A : \Ty)\{X_0\,X_1 : \Set\}(X^R : X_0 \to X_1 \to \Set) \to A^A\,X_0 \to A^A\,X_1 \to \Set$}\\
  & \hspace{-0.5em}\iota^R\,&&X^R\,\alpha_0\,\,\alpha_1 \equiv X^R\,\alpha_0\,\alpha_1 \\
  & \hspace{-0.5em}(\iota\to A)^R\,&&X^R\,\alpha_0\,\,\alpha_1 \equiv
       (x_0 : X_0)(x_1 : X_1) \to X^R\,x_0\,x_1 \to A^R\,X^R\,(\alpha_0\,x_0)\,(\alpha_1\,x_1)
\end{alignat*}
Here, functions are related if they map related inputs to related outputs. If we
know that $X^M\,\alpha_0\,\alpha_1 \equiv (f\,\alpha_0 = \alpha_1)$ for some $f$
function, we get
\[
  (x_0 : X_0)(x_1 : X_1) \to f\,x_0 = x_1 \to A^R\,X^R\,(\alpha_0\,x_0)\,(\alpha_1\,x_1)
\]
Now, we can simply substitute along the input equality proof in the above type,
to get the previous definition for $(\iota \to A)^M$:
\[
  (x_0 : X_0) \to A^R\,X^R\,(\alpha_0\,x_0)\,(\alpha_1\,(f\,x_0))
\]
This substitution along the equation is called ``singleton contraction'' in the
jargon of homotopy type theory \cite{hottbook}. The ability to perform contraction
here is at the heart of the \emph{strict positivity restriction} for inductive
signatures. Strict positivity in our setting corresponds to only having
first-order function types in signatures. If we allowed function domains to be
arbitrary types, in the definition of $(A \to B)^M$ we would only have a
black-box $A^M\,X^M : A^A\,X_0 \to A^A\,X_1 \to \Set$ relation, which is not
known to be given as an equality.

In Chapter \ref{chap:fqiit} we expand on this. As a preliminary summary:
although higher-order functions have relational interpretation, such relations
do not generally compose. What we eventually aim to have is a \emph{category} of
algebras and algebra morphisms, where morphisms do compose. We need a
\emph{directed} model of the theory of signatures, where every signature becomes
a category of algebras. The way to achieve this, is to prohibit higher-order
functions, thereby avoiding the polarity issues that prevent a directed
interpretation for general function types.

\subsection{Displayed Algebras}

At this point we do not yet have specification for induction principles. We use
the term \emph{displayed algebra} to refer to ``dependent'' algebras, where
every displayed algebra component lies over corresponding components in the base
algebra. For the purpose of specifying induction, displayed algebras can be
viewed as bundles of induction motives and methods.

Displayed algebras over some $\gamma : \Alg\,\Gamma$ are equivalent to slices
over $\gamma$ in the category of $\Gamma$-algebras; we show this in Chapter
\ref{chap:fqiit}. A slice $f : \Sub\,\Gamma\,\gamma'\,\gamma$ maps elements of
$\gamma$'s underlying set to elements in the base algebra. Why do we need
displayed algebras, then? The main reason is that if we are to eventually
implement inductive types in a dependently typed language, we need to compute
induction principles exactly, not merely up to isomorphisms.

For more illustration of using some displayed algebras in a type-theoretic
setting, see \cite{displayedcats}. We adapt the term ``displayed algebra'' from
ibid.\ as a generalization of displayed categories (and functors, natural
transformations) to other algebraic structures.

The displayed algebra interpretation is a \emph{logical predicate}
interpretation, defined as follows.
\begin{alignat*}{3}
  & \rlap{$\blank^D : (A : \Ty)\{X\} \to (X \to \Set) \to A^A\,X \to \Set$}\\
  & \iota^D\,       && X^D\,\alpha \equiv X^D\,\alpha \\
  & (\iota\to A)^D\,&& X^D\,\alpha \equiv (x : X)(x^D : X^D\,x) \to A^D\,X^D\,(\alpha\,x)\\
  & &&\\
  & \rlap{$\blank^D : (\Gamma : \Con)\{X\} \to (X \to \Set) \to \Gamma^A\,X \to \Set$}\\
  & \rlap{$\Gamma^D\,X^D\,\gamma \equiv
       \{A : \Ty\}(x : \Var\,\Gamma\,A) \to A^D\,X^D\,(\gamma\,x)$}\\
  & &&\\
  & \rlap{$\blank^D : (t : \Tm\,\Gamma\,A)
      \to \Gamma^D\,X^D\,\gamma \to A^D\,X^D\,(t^A\,\gamma)$}\\
  & (\var\,x)^D\,&&\gamma^D \equiv \gamma^D\,x\\
  & (\app\,t\,u)^D\,&&\gamma^D \equiv t^D\,\gamma^D\,(u^A\,\gamma)\,(u^D\,\gamma^D)\\
  & &&\\
  & \rlap{$\blank^D : (\sigma : \Sub\,\Gamma\,\Delta)
      \to \Gamma^D\,X^D\,\gamma \to \Delta^D\,X^D\,(\sigma^A\,\gamma)$}\\
  & \rlap{$\sigma^D\,\gamma^D\,x \equiv (\sigma\,x)^D\,\gamma^D$}
\end{alignat*}
Analogously to before, everything depends on a predicate interpretation $X^D : X
\to \Set$ for $\iota$. For types, a predicate holds for a function if the
function preserves predicates. The interpretation of terms is again a
fundamental lemma, and we again have pointwise definitions for contexts and
substitutions.
\begin{mydefinition}[Displayed algebras]
\begin{alignat*}{3}
  & \DispAlg : \{\Gamma : \Con\} \to \Alg\,\Gamma \to \Set_1\\
  & \DispAlg\,\{\Gamma\}\,(X,\,\gamma) \equiv (X^D : X \to \Set) \times \Gamma^D\,X^D\,\gamma
\end{alignat*}
\end{mydefinition}
\begin{myexample} We have the following computation.
\begin{alignat*}{3}
  & \hspace{-5em}\rlap{$\DispAlg\,\{\ms{NatSig}\}\,(X,\,\mi{zero},\,\mi{suc}) \equiv$}\\
              & (X^D &&: X \to \Set)\\
      \times\,& (\mi{zero^D} &&: X^D\,\mi{zero})\\
      \times\,& (\mi{suc^D} &&: (n : X) \to X^D\,n \to X^D\,(\mi{suc}\,n))
\end{alignat*}
\end{myexample}

\subsection{Sections}

Sections of displayed algebras are ``dependent'' analogues of algebra morphisms,
where the codomain is displayed over the domain.

\begin{alignat*}{3}
  & \hspace{-6em}\rlap{$\blank^S : (A : \Ty)\{X\,X^D\}(X^S : (x : X) \to X^D) \to (\alpha : A^A\,X) \to A^D\,X^D\,\alpha \to \Set$}\\
  & \hspace{-6em}\iota^S\,&&X^S\,\alpha\,\,\alpha^D \equiv X^S\,\alpha = \alpha^D \\
  & \hspace{-6em}(\iota\to A)^S\,&&X^S\,\alpha\,\,\alpha^D \equiv
  (x : X) \to A^S\,X^S\,(\alpha\,x)\,(\alpha^D\,(X^S\,x))\\
  & \hspace{-6em}&&\\
  &\hspace{-6em}\rlap{$\Con^S : (\Gamma : \Con)\{X\,X^D\}(X^S : (x : X) \to X^D) \to (\gamma : \Gamma^A\,X) \to \Gamma^D\,X^D\,\gamma \to \Set$}\\
  &\hspace{-6em}\rlap{$\Gamma^S\,X^S\,\gamma_0\,\gamma_1 \equiv
    \{A : \Ty\}(x : \Var\,\Gamma\,A) \to A^S\,X^S\,(\gamma_0\,x)\,(\gamma_1\,x)$}\\
  & \hspace{-6em} && \\
  & \hspace{-6em}\rlap{$\blank^S : (t : \Tm\,\Gamma\,A) \to \Gamma^S\,X^S\,\gamma\,\gamma^D \to A^S\,X^S\,(t^A\,\gamma)\,(t^D\,\gamma^D)$}\\
  & \hspace{-6em}(\var\,x)^S    &&\gamma^S \equiv \gamma^S\,x \\
  & \hspace{-6em}(\app\,t\,u)^S &&\gamma^S \equiv t^S\,\gamma^S\,(u^A\,\gamma)\\
  & \hspace{-6em}&& \\
  & \hspace{-6em}\rlap{$\blank^S : (\sigma : \Sub\,\Gamma\,\Delta) \to \Gamma^S\,X^S\,\gamma\,\gamma^D \to \Delta^S\,X^S\,(\sigma^A\,\gamma)\,(\sigma^A\,\gamma^D)$}\\
  & \hspace{-6em} \rlap{$\sigma^S\, \gamma^S\,x = (\sigma\,x)^S\,\gamma^S$}
\end{alignat*}

\begin{mydefinition}[Displayed algebra sections (``sections'' in short)]
\begin{alignat*}{3}
  & \Section : \{\Gamma : \Con\} \to (\gamma : \Alg\,\Gamma) \to \DispAlg\,\gamma \to \Set\\
  & \Section\,(X,\,\gamma)\,(X^D\,\gamma^D) \equiv (X^S : (x : X) \to X^D\,x) \times \Gamma^S\,X^S\,\gamma\,\gamma^D
\end{alignat*}
\end{mydefinition}
\begin{myexample} We have the following computation.
\begin{alignat*}{3}
  & \hspace{-5em}\rlap{$\Section\,\{\ms{NatSig}\}\,(X,\,\mi{zero},\,\mi{suc})\,(X^D,\,\mi{zero^D},\,\mi{suc^D}) \equiv$}\\
              & (X^S &&: (x : X) \to X^D\,x)\\
      \times\,& (\mi{zero^S} &&: X^S\,\mi{zero} = \mi{zero^D})\\
      \times\,& (\mi{suc^S} &&: (n : X) \to X^S\,(\mi{suc\,n}) = \mi{suc^D}\,n\,(X^S\,n))
\end{alignat*}
\end{myexample}

\begin{mydefinition}[Induction]
We define a predicate which holds if an algebra supports induction.
\begin{alignat*}{3}
  & \Inductive : \{\Gamma : \Con\} \to \Alg\,\Gamma \to \Set_1\\
  & \Inductive\,\{\Gamma\}\,\gamma \equiv
     (\gamma^D : \DispAlg\,\gamma) \to \Section\,\gamma\,\gamma^D
\end{alignat*}
\end{mydefinition}

We can observe that $\Inductive\,\{\ms{NatSig}\}\,(X,\,\ms{zero},\,\ms{suc})$
computes exactly to the usual induction principle for natural numbers. The input
$\DispAlg$ is a bundle of the induction motive and the methods, and the output
$\Section$ contains the $X^S$ eliminator function together with its $\beta$
computation rules.

\section{Term Algebras}

In this section we show that if a type theory supports the inductive types comprising
the theory of signatures, it also supports every inductive type which is described
by the signatures.

Note that we specified $\Tm$ and $\Sub$, but did not need either of them when
specifying signatures, or when computing induction principles. That signatures
do not depend on terms, is a property specific to simple signatures; this will
not be the case in Chapter \ref{chap:fqiit} when we move to more general
signatures. However, terms and substitutions are already useful here in the
construction of term algebras.

The idea is that terms in contexts comprise initial algebras. For example,
$\Tm\,\ms{NatSig}\,\iota$ is the set of natural numbers (up to
isomorphism). Informally, this is because the only way to construct terms is by
applying the $\ms{suc}$ variable (given by $\var\,\vz$) finitely many times to
the $\ms{zero}$ variable (given by $\var\,(\vs\,\vz)$).

\begingroup
\allowdisplaybreaks
\begin{mydefinition}[Term algebras]
Fix an $\Omega : \Con$. We abbreviate $\Tm\,\Omega\,\iota$ as $\ms{T}$; this will serve
as the carrier set of the term algebra. We additionally define the following.
\begin{alignat*}{3}
  & \hspace{-5em}\rlap{$\blank^T : (A : \Ty) \to \Tm\,\Omega\,A \to A^A\,\ms{T}$} \\
  & \hspace{-5em}\iota^T\,&&t \equiv t \\
  & \hspace{-5em}(\iota\to A)^T\,&&t \equiv \lambda\,u.\,A^T\,(\app\,t\,u)\\
  & \hspace{-5em}&& \\
  & \hspace{-5em}\rlap{$\blank^T : (\Gamma : \Con) \to \Sub\,\Omega\,\Gamma \to \Gamma^A\,\Gamma\,\ms{T}$}\\
  & \hspace{-5em}\rlap{$\Gamma^T\,\nu\,x \equiv A^T\,(\nu\,x)$}\\
  & \hspace{-5em}&& \\
  & \hspace{-5em}\rlap{$\blank^T : (t : \Tm\,\Gamma\,A)(\nu : \Sub\,\Omega\,\Gamma) \to A^T\,(t[\nu]) = t^A\,(\Gamma^T\,\nu)$}\\
  & \hspace{-5em}(\var\,x)^T\,   &&\nu    \text{   holds by   } \refl\\
  & \hspace{-5em}(\app\,t\,u)^T\,&&\nu \text {   holds by   } t^T\,\nu \text{   and   } u^T\,\nu\\
  & \hspace{-5em}&& \\
  & \hspace{-5em}\rlap{$\blank^T : (\sigma : \Sub\,\Gamma\,\Delta)(\nu : \Sub\,\Omega\,\Gamma)\{A\}(x : \Var\,\Delta\,A)$}\\
  & \hspace{-3em}\rlap{$\to \Delta^T\,(\sigma \circ \nu)\,x = \sigma^A\,(\Gamma^T\,\nu)\,x$}\\
  & \hspace{-5em}\rlap{$ \sigma^T\,\nu\,x \equiv (\sigma\,x)^T\,\nu$}
\end{alignat*}
Now we can define the term algebra for $\Omega$ itself:
\begin{alignat*}{3}
  & \TmAlg_{\Omega} : \Alg\,\Omega \\
  & \TmAlg_{\Omega} \equiv \Omega^T\,\Omega\,\id
\end{alignat*}
\end{mydefinition}
\endgroup

In the interpretation for contexts, it is important that $\Omega$ is
fixed, and we do induction on all $\Gamma$ contexts such that there is a
$\Sub\,\Omega\,\Gamma$. It would not work to try to compute term algebras by
direct induction on contexts, because we need to refer to the same $\ms{T}$ set
in the interpretation of every type in a signature.

The interpretation of types embeds terms as $A$-algebras. At the base type
$\iota$, this embedding is simply the identity function, since $\iota^A\,\ms{T}
\equiv \ms{T} \equiv \Tm\,\Omega\,\iota$. At function types we recursively proceed
under a semantic $\lambda$. The interpretation of substitutions is analogous.

The interpretations of terms and substitutions are coherence properties, which
relate the term algebra construction to term evaluation in the $\blank^A$ model.
For terms, if we pick $\nu \equiv \id$, we get $A^T\,t =
t^A\,\TmAlg_{\Omega}$. The left side embeds $t$ in the term model via
$\blank^T$, while the right hand side evaluates $t$ in the term model.

A way to view the term algebra construction, is that we are working in a
\emph{slice model} over the fixed $\Omega$, and every $\nu :
\Sub\,\Omega\,\Gamma$ can be viewed as an internal $\Gamma$-algebra in this
model. The term algebra construction demonstrates that
every such internal algebra yields an external element of $\Gamma^A$. We will
see in Section \ref{sec:fqiit-term-algebras} that we can construct term algebras
from \emph{any} model of a ToS, not just the ToS syntax; but while term algebras
constructed from ToS syntax are themselves initial algebras, in other cases they
may not be initial.

\subsection{Weak Initiality}
We show that $\TmAlg_{\Omega}$ supports a recursion principle, i.e.\ it is weakly
initial.

\begin{mydefinition}[Recursor construction] We assume $(X,\,\omega) : \Alg\,\Omega$;
recall that $X : \Set$ and $\omega : \Omega^A\,X$. We define $\ms{R} : \ms{T} \to X$
as $\ms{R}\,t \equiv t^A\,\omega$. We additionally define the following.
\begin{alignat*}{3}
& \hspace{-6em}\rlap{$\blank^R : (A : \Ty)(t : \Tm\,\Omega\,A) \to A^M\,\ms{R}\,(A^T\,t)\,(t^A\,\omega)$}\\
& \hspace{-6em}\iota^R\,&&t \equiv (\refl : t^A\,\omega = t^A\,\omega)\\
& \hspace{-6em}(\iota\to A)^R\,&&t \equiv \lambda\,u.\,A^R\,(\app\,t\,u)\\
& \hspace{-6em}&& \\
& \hspace{-6em}\rlap{$\blank^R : (\Gamma : \Con)(\nu : \Sub\,\Omega\,\Gamma) \to \Gamma^M\,\ms{R}\,(\Gamma^T\,\nu)\,(\nu^A\,\omega)$}\\
& \hspace{-6em}\rlap{$\Gamma^R\,\nu\,x \equiv A^R\,(\nu\,x)$}
\end{alignat*}
We define the recursor for $\Omega$ as
\begin{alignat*}{3}
  & \Rec_{\Omega} : (\mi{alg} : \Alg\,\Omega) \to \Mor\,\TmAlg_{\Omega}\,\mi{alg}\\
  & \Rec_{\Omega}\,(X,\,\omega) \equiv (\ms{R},\,\Omega^R\,\Omega\,\id)
\end{alignat*}
\end{mydefinition}

In short, the way we get recursion is by evaluating terms in arbitrary
$(X,\,\omega)$ algebras using $\blank^A$. The $\blank^R$ operation for types and
contexts confirms that $\ms{R}$ preserves structure appropriately, so that
$\ms{R}$ indeed yields algebra morphisms.

We skip interpreting terms and substitutions by $\blank^R$. It is necessary to
do so with more general signatures, but not in the current chapter.

\subsection{Induction}

We take the idea of the previous section a bit further. We have seen that
recursion for term algebras is given by evaluation in the ``standard'' model
$\blank^A$. Now, we show that induction for term algebras corresponds to
evaluation in the logical predicate model $\blank^D$.

\begin{mydefinition}[Eliminator construction]
We assume $(X^D,\,\omega^D) : \DispAlg\,\TmAlg_\Omega$. Recall that $X^D :
\ms{T} \to \Set$ and $\omega^D : \Omega^D\,X^D\,(\Omega^T\,\Omega\,\id)$. Like
before, we first interpret the underlying set:
\begin{alignat*}{3}
  & \ms{E} : (t : \ms{T}) \to X^D\,t \\
  & \ms{E}\,t \equiv t^D\,\omega^D
\end{alignat*}
However, this definition is not immediately well-typed, since $t^D\,\omega^D$
has type $X^D\,(t^A\,(\Omega^T\,\Omega\,\id))$, so we have to show that
$t^A\,(\Omega^T\,\Omega\,\id) = t$. This equation says that nothing happens if
we evaluate a term with type $\iota$ in the term model. We get it from the
$\blank^T$ interpretation of terms: $t^T\,\id : t[\id] =
t^A\,(\Omega^T\,\Omega\,\id)$, and we also know that $t[\id] = t$. We interpret types
and contexts as well:
\begin{alignat*}{3}
  & \hspace{-8em}\rlap{$\blank^E : (A : \Ty)(t : \Tm\,\Omega\,A) \to A^S\,\ms{E}\,(t^A\,(\Omega^T\,\Omega\,\id))\,(t^D\,\omega^D)$}\\
  & \hspace{-8em}\iota^E\,&&t : (t^A\,(\Omega^T\,\Omega\,\id))^D\,\omega^D = t^D\,\omega^D\\
  & \hspace{-8em}(\iota\to A)^E\,&&t \equiv \lambda\,u.\, A^E\,(\app\,t\,u)\\
  & \hspace{-8em}&& \\
  & \hspace{-8em}\rlap{$\blank^E : \Con : (\Gamma : \Con)(\nu : \Sub\,\Omega\,\Gamma) \to \Gamma^S\,\ms{E}\,(\nu^A\,(\Omega^T\,\Omega\,\id))\, (\nu^D\,\omega^D)$}\\
  & \hspace{-8em}\rlap{$\Gamma^E\,\nu\,x \equiv A^E\,(\nu\,x)$}
\end{alignat*}
In $\iota^E$ we use the same equation as in the definition of $\ms{E}$. In
$(\iota\to A)^E$ the definition is well-typed because of the same equation, but
instantiated for the abstracted $u$ term this time. All of this amounts to some
additional path induction and transport fiddling in the (intensional) Agda
formalization. We get induction for $\Omega$ as below.
\begin{alignat*}{3}
  &\Ind_{\Omega} : (\mi{alg} : \DispAlg\,\TmAlg_\Omega) \to \Section\,\TmAlg_\Omega\,\mi{alg}\\
  &\Ind_{\Omega}\,(X^D,\,\omega^D) \equiv (E,\, \Omega^E\,\Omega\,\id)
\end{alignat*}
\end{mydefinition}

\section{Discussion}

\subsection{Comparison to F-algebras}

A well-known alternative way for treating inductive types is to use certain
cocontinuous endofunctors as a more semantic notion of signatures.

For example, single-sorted inductive types can be presented as endofunctors
which preserve colimits of some ordinal-indexed chains. For instance, if we have
an $\kappa$-cocontinuous $F : \mbb{C} \to \mbb{C}$, then algebras are given as
$(X : |\mbb{C}|) \times (\mbb{C}(F\,X,\,X))$, morphisms as commuting squares,
and Adámek's theorem \cite{adamek} establishes the existence of initial
algebras.

An advantage of this approach is that we can describe different classes of
signatures by choosing different $\mbb{C}$ categories:
\begin{itemize}
  \item If $\mbb{C}$ is $\mbf{Set}$, we get simple inductive types.
  \item If $\mbb{C}$ is $\mbf{Set}^I$ for some set $I$, we get indexed inductive types.
  \item If $\mbb{C}$ is $\mbf{Set}/I$, we get inductive-recursive types.
\end{itemize}

Another advantage of $F$-algebras is that signatures are a fairly semantic
notion: they make sense even if we have no syntactic presentation at hand. That
said, often we do need syntactic signatures, for use in proof assistants, or
just to have a convenient notation for a class of cocontinuous functors.

An elegant way of carving out a large class of such functors is to
consider polynomials as signatures. For example, when working in \textbf{Set}, a
signature is an element of $(S : \Set) \times (P : S \to \Set)$, and $(S,\,P)$
is interpreted as a functor as $X \mapsto (s : S) \times (P\,s \to X)$. The
initial algebra is the W-type specified by $S$ shapes and $P$ positions. This
yields infinitary inductive types as well.

However, it is not known how to get \emph{inductive-inductive} signatures by
picking the right $\mbb{C}$ category and a functor. In an inductive-inductive
signature, there may be multiple sorts, which can be indexed over previously
declared sorts. For example, in the signature for categories we have $\Obj :
\Set$ and $\Mor : \Obj \to \Obj \to \Set$, indexed twice over $\Obj$. Some
extensions are required to the idea of $F$-algebras:
\begin{itemize}
\item
  For inductive-inductive definitions with two sorts, Forsberg gives a
  specification with two functors, and a considerably more complex notion of
  algebras, involving dialgebras \cite{forsberg-phd}.
\item
  For an arbitrary number of sorts, Altenkirch et
  al.\ \cite{altenkirch18qiit} use a ``list'' of functors, specified mutually
  with categories of algebras: each functor has as domain the semantic category
  of all previous sorts.
\end{itemize}

The functors-as-signatures approach gets significantly less convenient as we
consider more general specifications. The approach of this thesis is the skip the
middle ground between syntactic signatures and semantic categories of algebras:
we treat syntactic signatures as a key component, and give direct semantic
interpretation for them. Although we lose the semantic nature of $F$-algebras,
our approach scales extremely well, all the way up to infinitary
quotient-inductive-inductive types in Chapter \ref{chap:iqiit}, and to some
extent to higher inductive-inductive types as well in Chapter \ref{chap:hiit}.

If we look back at $\blank^A : \Con \to \Set \to \Set$, we may note that
$\Gamma^A$ yields a functor, in fact the same functor (up to isomorphism) that
we would get from an $F$-algebra presentation. However, this is a coincidence in
the single-sorted case. With the $F$-algebra presentation, we can view $(X :
|\mbb{C}|) \times (\mbb{C}(F\,X,\,X))$ as specifying the category of algebras as
the total category of a displayed category (by viewing the $\Sigma$-type here as
taking total categories; a $\Sigma$ in $\mbf{Cat}$). In our approach, we aim to
get the displayed categories directly, without talking about functors.

\subsection{Generic Programming}

Let's consider now our signatures and term algebras in the context of generic
programming. This is largely future work, and we don't elaborate it much. But we
can draw some preliminary conclusions and make some comparisons.

If a language can formalize inductive signatures and their semantics, that can
be viewed as an implementation of generic programming over the described types.
Compared to a purely mathematical motivation for this formalization, the
requirements for practical generic programming are a bit more stringent.
\begin{itemize}
  \item \emph{Encoding overhead}: there should be an acceptable overhead in
    program size and performance when using a generic representations.  Size
    blowup can be an issue when writing proofs as well, when types and
    expressions become too large to mentally parse.
  \item \emph{Strictness properties}: generic representations should compute as
    much as possible, ideally in exactly the same way as their non-generic
    counterparts.
\end{itemize}

\paragraph{Fixpoints of functors.}There is a sizable literature
of using fixpoints of functors in generic programming, mainly in Haskell
\cite{alacarte,compdata,multirec} and Agda \cite{loh11generic,allais20type}. We
give a minimal example below for an Agda-like implementation.

We have an inductive syntax for some strictly positive functors, covering essentially the
same signatures as $\Con$.
\begin{alignat*}{3}
  & \ms{Sig}                &&: \Set \\
  & \ms{Id}                 &&: \ms{Sig} \\
  & \ms{K\top}              &&: \ms{Sig} \\
  & \blank\!\otimes\!\blank &&: \ms{Sig} \to \ms{Sig} \to \ms{Sig} \\
  & \blank\!\oplus\!\blank  &&: \ms{Sig} \to \ms{Sig} \to \ms{Sig}
\end{alignat*}

$\ms{Id}$ codes the identity functor, and $\ms{K\top}$ codes the functor which
is constantly $\top$. $\blank\!\otimes\!\blank$ and $\blank\!\oplus\!\blank$ are
pointwise products and coproducts respectively. So we have the evident
interpretation functions:
\begin{alignat*}{3}
  & \llbracket\blank\rrbracket &&: \ms{Sig} \to \Set \to \Set\\
  & \ms{map} &&: (F : \ms{Sig}) \to (X_0 \to X_1) \to \llbracket F \rrbracket\,X_0 \to \llbracket F \rrbracket\,X_1
\end{alignat*}
In Haskell and Agda, the easiest way to get initial algebras is to directly define
the inductive fixpoint for each assumed $F : \ms{Sig}$:
\begin{alignat*}{3}
  & \ms{Fix_F} &&: \Set \\
  & \ms{con_F} &&: \llbracket F \rrbracket\,\ms{Fix_F} \to \ms{Fix_F}
\end{alignat*}
In Haskell, this definition is valid for arbitrary \emph{semantic} $F$ functor,
because there is no termination checking, and thus no positivity checking. In
Agda, the above definition is valid because the positivity checker looks inside
the definition of $\llbracket\blank\rrbracket$ and lets it through. Next we establish
weak initiality, by defining the recursor:
\begin{alignat*}{3}
  & \ms{rec} : (\llbracket F \rrbracket\,X \to X) \to \ms{Fix_F} \to X \\
  & \ms{rec}\,f\,(\ms{con}\,x) \equiv f\,(\ms{map}\,(\ms{rec}\,f)\,x)
\end{alignat*}
This is again fine in Haskell, but it unfortunately does not pass Agda's
termination checker. There are several possible solutions, assuming that we
stick to functors as signatures:
\begin{enumerate}
  \item Using \emph{sized types} \cite{abel17normalization}. For example, this
    was used in \cite{allais20type}. The drawback is dependence on an extra
    language feature which is only supported in Agda, out of the major
    dependently typed languages.
  \item Turning termination checking off.
  \item Not using the direct inductive definition for fixpoints, and thereby not
    being exposed to the whims of syntactical strict positivity checking in
    Agda. Instead, defining initial algebras as sequential colimits, using
    Adámek's theorem. This approach was taken by Ahrens, Matthes and Mörtberg in
    \cite{ahrens19from}. However, the encoding overhead of this approach is excessive, and
    it is practically unusable for generic programming. Another drawback is that
    defining colimits requires quotient types, which are often not available natively.
\end{enumerate}

\paragraph{W-types.} Given a polynomial $(S,\,P) : (S : \Set) \times (S \to \Set)$,
the corresponding W-type is inductively specified as below:
\begin{alignat*}{3}
  & \ms{W}_{S,\,P} &&: \Set \\
  & \ms{sup} &&: (s : S) \to (P\,s \to \ms{W}_{S,\,P}) \to \ms{W}_{S,\,P}
\end{alignat*}
If we assume $\top$, $\bot$, $\Bool$, $\Pi$, $\Sigma$, $\ms{W}$-types, universes
and an intensional identity type, a large class of inductive types can be
derived, including infinitary and finitary indexed inductive families; this
was shown by Hugunin \cite{whynotw}. The encoding also yields definitional
$\beta$-rules for recursion and elimination. However, there is also significant
encoding overhead here.
\begin{itemize}
  \item
    First, there is a translation from more convenient signatures to $(S,\,P)$ polynomials.
  \item Then, we take the $\ms{W}_{S\,P}$ type, but we need to additionally
    restrict it to the \emph{canonical} elements by a predicate, as in $(x :
    \ms{W}_{S\,P}) \times \ms{canonical}\,x$. This is required because the only
    way to represent inductive branching is by functions, but functions
    sometimes contain too many elements up to definitional equality. For
    example, $\bot \to A$ has infinitely many definitonally distinct
    inhabitants.
\end{itemize}
There is also a performance overhead imposed by the mandatory higher-order
constructors. W-types are a great way to have a small basis in a formal setting,
both in intensional and extensional type theories, but they are a bit too heavy
for practical purposes.

\paragraph{Term algebras.}
The term algebras described in this chapter compare quite favorably. As we have
seen, induction for term algebras can be defined in a small amount of easy code,
without using sized types or quotients.

For practical usefulness, it makes sense to make a slight modification of
terms. We switch to a \emph{spine neutral} definition. We mutually inductively
define $\ms{Spine}$ and $\Tm$:
\begin{alignat*}{3}
  & \ms{Spine} &&: (\Gamma : \Con) \to \Ty \to \Ty \to \Set \\
  & \epsilon &&: \{A\} \to \ms{Spine}\,\Gamma\,A\,A\\
  & \blank\!,\!\blank &&: \{B\,C\} \to \Tm\,\Gamma\,\iota \to \ms{Spine}\,\Gamma\,B\,C \to \ms{Spine}\,\Gamma\,(\iota\to B)\,C\\
  & \Tm  &&: \Con \to \Ty \to \Set \\
  & \blank\$\blank &&: \{A\,B\} \to \Var\,\Gamma\,A \to \ms{Spine}\,\Gamma\,A\,B \to \Tm\,\Gamma\,B
\end{alignat*}
With this representation, a term is always a variable applied to a list of
arguments. This can be useful, because the pattern matching implementation of
the metalanguage knows about which constructors are possible, when matching
on values of $\Tm\,\Gamma\,A$. Using this, we give here an ad-hoc definition of
natural numbers in pseudo-Agda.
\begin{alignat*}{3}
  &\Nat : \Set && \ms{zero} &&\equiv \vz\,\$\,\epsilon\\
  &\Nat \equiv \Tm\,(\emptycon\ext \iota\to \iota \ext \iota)\,\iota\hspace{2em} && \ms{suc}\,n &&\equiv \vs\,\vz\,\$\,(n,\,\epsilon)
\end{alignat*}
\begin{alignat*}{3}
  &\hspace{-6.5em}\rlap{$\ms{NatElim} : (P : \Nat \to \Set) \to P\,\ms{zero}\to ((n : \Nat) \to P\,n \to P\,(\ms{suc}\,n))$}\\
  &\hspace{-6.5em}\rlap{$\hspace{4em}\to (n : \Nat) \to P\,n$}\\
  &\hspace{-6.5em}\ms{NatElim}\,P\,\ms{pz}\,\ms{ps}\,(\vz\,\$\,\epsilon) &&\equiv \ms{pz}\\
  &\hspace{-6.5em}\ms{NatElim}\,P\,\ms{pz}\,\ms{ps}\,(\vs\,\vz\,\$\,(n,\,\epsilon)) &&\equiv
    \ms{ps}\,n\,(\ms{NatElim}\,P\,\ms{pz}\,\ms{ps}\,n)
\end{alignat*}
The actual Agda definition can be found in the supplementary formalization, and
it is pretty much the same as above. We recover the exact same behavior with
respect to pattern matching as with native inductive definitions.

\todo{formalize Rec and Ind with spines in Agda}

$\Rec_\Omega$ and $\Ind_\Omega$ can be adapted to spine neutral terms with minor
adjustments. But what about the $\beta$-rules which they produce as part of
their output, are they definitional (i.e.\ proven by $\refl$)? Informally, it is
easy to see that any concrete definition of an eliminator like $\ms{NatElim}$
enjoys definitional $\beta$. In this chapter we do not have a nice way of
reasoning about definitional equalities. In the next chapter we develop such
reasoning and show that an $\Ind_\Omega$ which is defined internally to an
intensional theory (as in this chapter) only has propositional $\beta$-rules,
but if it is defined in a metaprogramming layer, it has definitional $\beta$,
even if we don't use spine neutral terms.

The term algebra presentation can be easily extended to indexed families. In
that case, signatures and terms are still definable with basic inductive
families, without requiring quotients or complicated encodings; see Kaposi and
von Raumer \cite{mutualinductive}.

The closest existing work is \emph{sum-of-products} generics by De Vries and Löh
\cite{sop}. There, signatures for functors are in a normal form: we cannot
freely take products and coproducts, instead a signature looks very much like a
$\Con$ in this chapter (except in an indexed form). The authors observe that
several generic programming patterns are easier to express with normalized
signatures. However, they still use explicit fixpoints as the way to get initial
algebras.

Hence, it remains future work to see how term algebras might be used in more
practically-oriented generic programming.


\chapter{Semantics in Two-Level Type Theory}
\label{chap:2ltt}

In this chapter we describe how two-level type theory is used as a metatheoretic
setting in the rest of this thesis. First, we provide motivation and
overview. Then, we introduce two-level type theory more formally and describe its
intended models. Finally, we generalize the semantics and the term algebra
construction from Chapter \ref{chap:simple-inductive-signatures} in type-level
type theory, as a way to illustrate the applications.

\section{Motivation}
\label{sec:2ltt-motivation}
We note two shortcomings of the semantics presented in the
previous chapter.

First, we were not able to reason about definitional equalities, only
propositional ones. We have a formalization of signatures and semantics in
intensional Agda, where the two notions differ\footnote{As opposed to in
extensional type theory, where they're the same.}, but only propositional
equality is subject to internal reasoning. For instance, we would like to show
that term algebras support recursion and elimination with strict $\beta$-rules,
and for this we need to reason about strict equality.

Second, the semantics that we provided was not as general as it could be. We
used the internal $\Set$ universe to specify algebras, but algebras make sense
in many different categories. A crude way to generalize semantics is to simply
say that our formalization, which was carried out in the syntax (i.e.\ initial
model) of some intensional type theory, can be interpreted in any model of the
type theory. However, this is wasteful: for simple inductive signatures, it is
enough to assume a category with finite products as semantic setting. We don't
need all the extra baggage that comes with a model of intensional type theory.

\begin{notation}
We use $\emptycon$ for the terminal object in a $\mbb{C}$ category, with
$\epsilon : \mbb{C}(A,\,\emptycon)$ for the unique morphism. For products, we
use $\blank\!\otimes\!\blank$ with $(\blank\!,\!\blank) : \mbb{C}(A,\,B) \to
\mbb{C}(A,\,C) \to \mbb{C}(A,\,B\otimes C)$ and $\pi_1$ and $\pi_2$ for
projections.
\end{notation}

\begin{myexample}
Assuming $\mbbC$ has finite products, natural number algebras and binary tree
algebras are specified as follows.
\begin{alignat*}{3}
  &\Alg_{\ms{NatSig}} &&\equiv (X : |\mbbC|) \times \mbbC(\emptycon,\,X) \times \mbbC(X,\,X)\\
  &\Alg_{\ms{TreeSig}}&&\equiv (X : |\mbbC|) \times \mbbC(\emptycon,\,X) \times \mbbC(X \otimes X,\,X)
\end{alignat*}
\end{myexample}
Here, $\Alg_{\ms{NatSig}}$ and $\Alg_{\ms{TreeSig}}$ are both sets in an
unspecified metatheory, and the $\times$ in the definitions refer to the
metatheoretic $\Sigma$. The algebras can be viewed as diagrams which preserve
finite products, and algebra morphisms become natural transformations.

How should we adjust $\Alg$ to compute algebras in $\mbbC$, and $\Mor$ to
compute their morphisms? While it is possible to do this in a direct fashion,
working directly with objects and morphisms of $\mbbC$ is rather unwieldy. $\mbbC$ is
missing many convenience features of type theories.
\begin{itemize}
\item There are no variables or binders. We are forced to work in a point-free
  style or chase diagrams; both become difficult to write and read after a
  certain point of complexity.
\item
  There no functions, universes or inductive types.
\item
  Substitution (with weakening as a special case) has to be handled explicitly
  and manually. Substitutions are certain morphisms, while ``terms'' are also
  morphisms, and we have to use composition to substitute terms. In contrast, if
  we are working internally in a type theory, terms and substitutions are
  distinct, and we only have to explicitly deal with terms, and substitutions
  are automated and implicit.
\end{itemize}

The above overlaps with motivations for working in \emph{internal languages}
\cite{internallogic} of structured categories: they aid calculation and compact
formalization by hiding bureaucratic structural details.

A finite product category $\mbbC$ does not have much of an internal language: it
is too bare-bones. But we can work instead in the internal language of
$\hat\mbbC$, the category of presheaves over $\mbbC$. This allows faithful
reasoning about $\mbbC$, while also including all convenience features of
extensional type theory.

\emph{Two-level type theories} \cite{twolevel}, or 2LTT in short, are type
theories such that they have ``standard'' interpretations in presheaf
categories. A 2LTT has an inner layer, where types and terms arise by embedding
$\mbbC$ in $\hat{\mbbC}$, and an outer layer, where constructions are inherited
from $\hat{\mbbC}$. The exact details of the syntax may vary depending on what
structures $\mbbC$ supports, and which type formers we assume in the outer
layer. Although it is possible to add assumptions to a 2LTT which preclude
standard presheaf semantics \cite[Section 2.4.]{twolevel}, we stick to basic
2LTT in this thesis. By using 2LTT, we are able to use a type-theoretic syntax
in the rest of the thesis which differs only modestly from the style of
definitions that we have seen so far.

From a programming perspective, basic 2LTT provides a convenient syntax for
writing metaprograms. This can be viewed as \emph{two-stage compilation}: if we
have a 2LTT program with an inner type, we can run it, and it returns another
program, which lives purely in the inner theory.

\section{Models of 2LTT}

We take an algebraic view \cite{TODO} of models and syntaxes of type theories throughout
this thesis. Models of type theories are algebraic structures: they are
categories with certain extra structure. The syntax of a type theory is
understood to be its initial model. In initial models, the underlying category
is the category of typing contexts and parallel substitutions, while the extra
structure corresponds to type and term formers, and equations quotient the
syntax by definitional equality.

Type theories can be described with quotient inductive-inductive (QII)
signatures, and their initial models are quotient inductive-inductive types
(QIITs). Hence, 2LTT is also a QII theory. We will first talk about QIITs in
Chapter \ref{chap:fqiit}. Until then, we shall make do with an informal
understanding of categorical semantics for type theories, without using anything
in particular from the metatheory of QIITs. There is some annoying circularity
here, that we talk about QIITs in this thesis, but we employ QIITs when talking
about them. However, this is only an annoyance in exposition and not a
fundamental issue: Chapter \ref{chap:levitation} describes how to eliminate
circularity by a form of bootstrapping.

The algebraic view lets us dispense with all kinds of ``raw'' syntactic objects.
We only ever talk about well-typed and well-formed objects, moreover, every
construction must respect definitional equalities. For terms in the algebraic
syntax, definitional equality coincides with metatheoretic equality. This
mirrors equality of morphisms in 1-category theory, where we usually reuse
metatheoretic equality.

\subsection{Models of Type Theories}

Before elaborating on 2LTT-specific features, we review models of type theories
in general. Variants of 2LTT will be obtained by adding extra features on the
top of more conventional TTs.

It is also worth to take a look at models of type theories at this point,
because the notions presented in this subsection (categories with families, type
formers) will be reused several times in this thesis, when specifying theories
of signatures.

\subsubsection{Categories With Families}

\begin{mydefinition}
A \emph{category with family} (cwf) \cite{Dybjer96internaltype} is a way to
specify the basic structural rules for contexts, substitutions, types and
terms. It yields a dependently typed explicit substitution calculus \cite{TODO}.
A cwf consists of the following.
\begin{itemize}
\item
  A category with a terminal object. We denote the set of objects as $\Con :
  \Set$ and use capital Greek letters starting from $\Gamma$ to refer to
  objects. The set of morphisms is $\Sub : \Con \to \Con \to \Set$, and we use
  $\sigma$, $\delta$ and so on to refer to morphisms. We write $\id$ for the
  identity morphism and $\blank\circ\blank$ for composition. The terminal
  object is $\emptycon$ with unique morphism $\epsilon :
  \Sub\,\Gamma\,\emptycon$. In initial models (that is, syntaxes) of type
  theories, objects correspond to typing contexts, morphisms to parallel
  substitutions and the terminal object to the empty context; this informs the
  naming scheme.
\item A \emph{family structure}, containing $\Ty : \Con \to \Set$ and $\Tm :
  (\Gamma : \Con) \to \Ty\,\Gamma \to \Set$. We use $A$, $B$, $C$ to refer to
  types and $t$, $u$, $v$ to refer to terms. $\Ty$ is a presheaf over the
  category of contexts and $\Tm$ is a displayed presheaf over $\Ty$. This means
  that types and terms can be substituted:
  \begin{alignat*}{3}
    &\blank[\blank] : \Ty\,\Delta \to \Sub\,\Gamma\,\Delta \to \Ty\,\Gamma\\
    &\blank[\blank] : \Tm\,\Delta\,A \to (\sigma : \Sub\,\Gamma\,\Delta) \to \Tm\,\Delta\,(A[\sigma])
  \end{alignat*}
  Substitution is functorial: we have $A[\id] = A$ and
  $A[\sigma\circ\delta] = A[\sigma][\delta]$, and likewise for terms.

  A family structure is additionally equipped with \emph{context comprehension}
  which consists of a context extension operation $\blank\ext\blank : (\Gamma :
  \Con) \to \Ty\,\Gamma \to \Con$ together with an isomorphism
  $\Sub\,\Gamma\,(\Delta\ext A) \simeq ((\sigma : \Sub\,\Gamma\,\Delta) \times
  \Tm\,\Gamma\,(A[\sigma]))$ which is natural in $\Gamma$.
\end{itemize}
\end{mydefinition}

The following notions are derivable from the comprehension structure:
\begin{itemize}
\item
  By going right-to-left along the isomorphism, we recover \emph{substitution
  extension} $\blank,\blank : (\sigma : \Sub\,\Gamma\,\Delta) \to
  \Tm\,\Gamma\,(A[\sigma]) \to \Sub\,\Gamma\,(\Delta\ext A)$. This means that
  starting from $\epsilon$ or the identity substitution $\id$, we can iterate
  $\blank,\blank$ to build substitutions as lists of terms.
\item
  By going left-to-right, and starting from $\id : \Sub\,(\Gamma\ext
  A)\,(\Gamma\ext A)$, we recover the \emph{weakening substitution} $\p :
  \Sub\,(\Gamma\ext A)\,\Gamma$ and the \emph{zero variable} $\q :
  \Tm\,(\Gamma\ext A)\,(A[\p])$.
\item
  By weakening $\q$, we recover a notion of variables as De Bruijn indices. In
  general, the $n$-th De Bruijn index is defined as $\q[\p^{n}]$, where $\p^{n}$
  denotes $n$-fold composition.
\end{itemize}

Comprehension can be characterized either by taking $\blank,\blank$, $\p$ and
$\q$ as primitive, or the natural isomorphism. The two are equivalent, and we
may switch between them, depending on which is more convenient.

There are other ways for presenting the basic categorical structure of models,
which are nonetheless equivalent to cwfs, including natural models
\cite{awodey18natural} and categories with attributes \cite{cartmellthesis}. We
use the cwf presentation for its immediately algebraic character and closeness
to conventional explicit substitution syntax.

\begin{notation}As De Bruijn indices are hard to read, we will mostly use
nameful notation for binders. For example, assuming $\Nat : \{\Gamma : \Con\}
\to \Ty\,\Gamma$ and $\Id : \{\Gamma : \Con\}(A : \Ty\,\Gamma) \to
\Tm\,\Gamma\,A \to \Tm\,\Gamma\,A \to \Ty\,\Gamma$, we may write $\emptycon \ext
n : \Nat \ext p : \Id\,\Nat\,n\,n$ for a typing context, instead of using
numbered variables or cwf combinators as in $\emptycon \ext \Nat \ext
\Id\,\Nat\,\q\,\q$.
\end{notation}

\begin{notation}
In the following, we will denote family structures by ($\Ty$,$\Tm$) pairs and overload context
extension $\blank\ext\blank$ for different families.
\end{notation}

\begin{mydefinition} The following derivable operations are commonly required.
  \begin{itemize}
    \item \emph{Single substitution} can be derived from parallel substitution
      as follows. Assume $t : \Tm\,(\Gamma\ext A)\,B$, and $u :
      \Tm\,\Gamma\,A$. $t$ is a term which may depend on the last variable in
      the context, which has $A$ type. We can substitute that variable with the
      $u$ term as $t[\id,\,u] : \Tm\,\Gamma\,(\A[\id,\,u])$. Note that term
      substitution causes the type to be substituted as well. $(\id,\,u) :
      \Sub\,\Gamma\,(\Gamma\ext A)$ is well-typed because $u : \Tm\,\Gamma\,A$
      hence also $u : \Tm\,\Gamma\,(A[\id])$.

    \item We can \emph{lift substitutions} over binders as follows. Assuming
      $\sigma : \Sub\,\Gamma\,\Delta$ and $A : \Ty\,\Delta$, we construct a
      lifting of $\sigma$ which maps an additional $A$-variable to itself:
      $(\sigma\circ\p,\,\q) : \Sub\,(\Gamma\ext A[\sigma])\,(\Delta \ext A)$.
      Let us see why this is well-typed. We have $\p : \Sub\,(\Gamma\ext
      A[\sigma])\,\Gamma$ and $\sigma : \Sub\,\Gamma\,\Delta$, so $\sigma \circ
      \p : \Sub\,(\Gamma\ext A[\sigma])\,\Delta$. Also, $\q : \Tm\,(\Gamma\ext
      A[\sigma])\,(A[\sigma][\p])$, hence $\q : \Tm\,(\Gamma\ext
      A[\sigma])\,(A[\sigma \circ \p])$, thus $(\sigma\circ \p,\,\q)$
      typechecks.
  \end{itemize}
\end{mydefinition}

\begin{notation}

As a nameful notation for substitutions, we may write $t[x \mapsto u]$, for
a single substitution, or $t[x \mapsto u_1, y \mapsto u_2]$ and so on.

In nameful notation we leave all weakening implicit, including substitution
lifting. Formally, if we have $t : \Tm\,\Gamma\,A$, we can only mention $t$ in
$\Gamma$. If we need to mention it in $\Gamma \ext B$, we need to use $t[\p]$
instead. In the nameful notation, $t : \Tm\,(\Gamma\ext x : B)\,A$ may be
used.\footnote{Moreover, when working in the internal syntax of a theory, we
just write Agda-like type-theoretic notation, without noting contexts and
substitutions in any way.}
\end{notation}

\subsubsection{Type formers}
A family structure in a cwf may be closed under certain type formers, such as
functions, $\Sigma$-types, universes or inductive types. We give some examples
here for their specification. First, we look at common negative type formers,
which can be specified using isomorphisms. Then, we consider positive type
formers, and finally universes.

\subsubsection{Negative types}

\begin{mydefinition}[$\Pi$-types] A $(\Ty,\,\Tm)$ family supports $\Pi$ types if it supports the following.
\begin{alignat*}{3}
  &\Pi           &&: (A : \Ty\,\Gamma) \to \Ty\,(\Gamma\ext A) \to \Ty\,\Gamma\\
  &\ms{\Pi[]}    &&: (\Pi\,A\,B)[\sigma] = \Pi\,(A[\sigma])\,(B[\sigma\circ\p,\,\q])\\
  &\app          &&: \Tm\,\Gamma\,(\Pi\,A\,B) \to \Tm\,(\Gamma \ext A)\,B\\
  &\lam          &&: \Tm\,(\Gamma \ext A)\,B \to \Tm\,\Gamma\,(\Pi\,A\,B)\\
  &\Pi\beta      &&: \app\,(\lam\,t) = t\\
  &\Pi\eta       &&: \lam\,(\app\,t) = t\\
  &\lam[]        &&: (\lam\,t)[\sigma] = \lam\,(\sigma\circ\p,\,\q)
\end{alignat*}
Here, $\Pi$ is the type formation rule. $\ms{\Pi[]}$ is the type substitution
rule, expressing that substituting $\Pi$ proceeds structurally on constituent
types.  Note $B[\sigma\circ\p,\,\q]$, where we lift $\sigma$ over the additional
binder.

The rest of the rules specify a natural isomorphism $\Tm\,\Gamma\,(\Pi\,A\,B)
\simeq \Tm\,(\Gamma \ext A)\,B$. We only need a substitution rule (i.e.\ a
naturality rule) for one direction of the isomorphism, since the naturality of
the other map can is derivable.

This way of specifying $\Pi$ types is very convenient if we have explicit
substitutions. The usual ``pointful'' specification is equivalent to this. For
example, we have the following derivation of pointful application:
\begin{alignat*}{3}
  &\app' : \Tm\,\Gamma\,(\Pi\,A\,B) \to (u : \Tm\,\Gamma\,A) \to \Tm\,\Gamma\,(B[\id,\,u])\\
  &\app'\,t\,u \equiv (\app\,t)[\id,\,u]
\end{alignat*}

\end{mydefinition}

\textbf{Remark on naturality.} The above specification for $\Pi$ can be written
more compactly if we assume that everything is natural with respect to
substitution.
\begin{alignat*}{3}
  &\Pi            &&: (A : \Ty\,\Gamma) \to \Ty\,(\Gamma\ext A) \to \Ty\,\Gamma\\
  & (\app,\,\lam) &&: \Tm\,\Gamma\,(\Pi\,A\,B) \simeq \Tm\,(\Gamma \ext A)\,B
\end{alignat*}
This is a reasonable assumption; in the rest of the thesis we only ever define
structures on cwfs which are natural in this way.

\begin{notation} From now on, when specifying type formers in family structures,
we assume that everything is natural, and thus omit substitution equations.
\end{notation}

There are ways to make this idea more precise, and take it a step further by
working in languages where only natural constructions are possible. The term
\emph{higher-order abstract syntax} is sometimes used for this style. It lets us
also omit contexts, so we would only need to write
\begin{alignat*}{3}
  &\Pi            &&: (A : \Ty) \to (\Tm\,A \to \Ty) \to \Ty\\
  & (\app,\,\lam) &&: \Tm\,(\Pi\,A\,B) \simeq ((a : \Tm\,A) \to \Tm\,(B\,a))
\end{alignat*}
Recently several promising works emerged in this area \cite{TODO}. Although this
technology is likely to be the preferred future direction in the metatheory of
type theories, this thesis does not make use of it. The field is rather fresh,
with several different approaches and limited amount of pedagogical exposition,
and the new techniques would also raise the level of abstraction in this thesis,
all contributing to making it less accessible. One more reason, and perhaps the
most important one, for skipping higher-order abstract syntax, is that this
author has not yet acquired the proficiency to comfortably use the new
techniques.

\begin{mydefinition}[$\Sigma$-types] A family structure supports $\Sigma$-types if we have
\begin{alignat*}{3}
  & \Sigma  &&: (A : \Ty\,\Gamma) \to \Ty\,(\Gamma\ext A) \to \Ty\,\Gamma\\
  & (\proj,\,(\blank,\blank)) &&: \Tm\,\Gamma\,(\Sigma\,A\,B) \simeq ((t : \Tm\,\Gamma\,A) \times \Tm\,\Gamma\,(B[\id,\,t]))
\end{alignat*}
We use the shorter specification above, where everything is assumed to be
natural. We may write $\proj_1$ and $\proj_2$ for composing the metatheoretic
first and second projections with $\proj$.
\end{mydefinition}

\begin{mydefinition}[Unit type]
A family structure supports the unit type if we have $\top : \Ty\,\Gamma$ such
that $\Tm\,\Gamma\,\top \simeq \top$, where the $\top$ on the right is the
metatheoretic unit type, and we overload $\top$ for the internal unit type.
From this, we get the internal $\tt : \Tm\,\Gamma\,\top$, which is
definitionally unique.
\end{mydefinition}

The identity type can be also specified as a negative type.
In this case we get an extensional identity, while if we define it as a
positive type, we get intensional identity instead.

This choice between negative and positive specification generally exists for
type formers with a single term construction rule. For example, $\Sigma$ can be
defined as a positive type, with an elimination rule that behaves like pattern
matching. Positive $\Sigma$ is equivalent to negative $\Sigma$, although it only
supports propositional $\eta$-rules. In contrast, positive identity is usually
\emph{not} equivalent to negative identity.

\begin{mydefinition}[Extensional identity]
A family structure supports extensional identity types if there is $\Id :
\Tm\,\Gamma\,A \to \Tm\,\Gamma\,A \to \Ty\,\Gamma$ such that
$(\reflect,\,\refl) : \Tm\,\Gamma\,(\Id\,t\,u) \simeq (t = u)$.
\end{mydefinition}

$\refl : t = u \to \Tm\,\Gamma\,(\Id\,t\,u)$ expresses reflexivity of identity:
definitionally equal terms are provably equal. $\reflect$, which goes the other
way around, is called \emph{equality reflection} \cite{TODO}: provably equal
terms are identified in the metatheory.

Uniqueness of identity proofs (UIP) is often ascribed to the extensional
identity type \cite{TODO}. UIP means that $\Tm\,\Gamma\,(\Id\,t\,u)$ has at most
a single inhabitant up to $\Id$. However, UIP is not something which is inherent
in the negative specification, instead it's inherited from the metatheory. If
$\Tm$ forms a set in the metatheory (by assumption, or by not having
higher-dimensional types in the metatheory), then internal equality proofs
inherit uniqueness through the defining isomorphism.

\subsubsection{Positive types}

We do not dwell much on positive types here, as elsewhere in this thesis we talk
a lot about specifying such types anyway. We provide here some background and
a small example.

The motivation is to specify initial internal algebras in a cwf. However,
specifying the uniqueness of recursors using definitional equality is
problematic, if we are to have decidable and efficient conversion checking for a
type theory. Consider the specification of $\Bool$ together with its recursor.
\begin{alignat*}{3}
  & \Bool  &&: \Ty\,\Gamma \\
  & \true  &&: \Tm\,\Gamma\,\Bool \\
  & \false &&: \Tm\,\Gamma\,\Bool \\
  & \ms{BoolRec} &&: (B : \Ty\,\Gamma)\to \Tm\,\Gamma\,B \to \Tm\,\Gamma\,B \to \Tm\,\Gamma\,\Bool \to \Tm\,\Gamma\,B\\
  & \true\beta &&: \ms{BoolRec}\,B\,t\,f\,\true = t\\
  & \false\beta &&: \ms{BoolRec}\,B\,t\,f\,\false = f
\end{alignat*}
$\ms{BoolRec}$ together with the $\beta$-rules specifies an internal
$\Bool$-algebra morphism. A possible way to specify definitional uniqueness is
as follows. Assuming $B : \Ty\,\Gamma$, $t : \Tm\,\Gamma\,B$, $f :
\Tm\,\Gamma\,B$ and $m : \Tm\,(\Gamma\ext b : \Bool)\,B$, such that $m[b \mapsto
  \true] = t$ and $m[b \mapsto \false] = f$, it follows that
$\ms{BoolRec}\,B\,t\,f\,b : \Tm\,(\Gamma\ext b : \Bool)\,B$ is equal to $m$.

Unfortunately, deciding conversion with this rule entails deciding pointwise
equality of arbitrary $\Bool$ functions, which can be done in exponential time
in the number of $\Bool$ arguments. More generally, Scherer presented a decision
algorithm for conversion checking with strong finite sums and products in simple
type theory \cite{scherer17deciding}, which also takes exponential time. If we
move to natural numbers with definitionally unique recursion, conversion
checking becomes undecidable.

One solution is to have propositionally unique recursion instead. However, if
such equations are postulated, that would break the canonicity property of basic
intensional type theory, since now we would have equality proofs besides $\refl$
in the empty context.

The standard solution is to have dependent elimination principles instead: this
allows inductive reasoning, canonicity and effectively decidable definitional
equality at the same time. For $\Bool$, we would have
\begin{alignat*}{3}
  & \ms{BoolInd} &&: (B : \Ty\,(\Gamma\ext b : \Bool)) \to \Tm\,\Gamma\,(B[b \mapsto \true])\\
  & &&\to \Tm\,\Gamma\,(B[b \mapsto \false]) \to (t : \Tm\,\Gamma\,\Bool) \to \Tm\,\Gamma\,(B[b \mapsto t])
\end{alignat*}
together with $\ms{BoolInd}\,B\,t\,f\,\true = t$ and $\ms{BoolInd}\,B\,t\,f\,\false = f$.

Of course, if we assume extensional identity types, we have undecidable
conversion anyway, and definitionally unique recursion is equivalent to
induction. But decidable conversion is an essential part of type theory, perhaps
its main selling point as a foundation for mechanized mathematics, which makes
it possible to relegate a deluge of boilerplate to computers. Hence, decidable
conversion should be kept in mind.

\subsubsection{Universes}

\todo{TODO}
























%% --------------------------------------------------------------------------------

\chapter{Finitary Quotient Inductive-Inductive Types}
\label{chap:fqiit}

\section{Theory of Signatures}
\label{sec:fqiit-tos}

\subsection{Models}
\subsection{Examples}

\section{Semantics}
\label{sec:fqiit-semantics}
\subsection{Finite Limit Cwfs}
\subsection{Equivalence of Initiality and Induction}
\subsection{Model of the Theory of Signatures}

\section{Term Algebras}
\label{sec:fqiit-term-algebras}
\subsection{Generic Term Algebras}
\subsection{Induction for Term Algebras}
\subsection{Church Encoding}
\subsection{Awodey-Frey-Speight Encoding}

\section{Left Adjoints of Signature Morphisms}

\chapter{Infinitary Quotient Inductive-Inductive Types}
\label{chap:iqiit}
\section{Theory of Signatures}
\section{Semantics}
\section{Term Algebras}

\chapter{Levitation, Bootstrapping and Universe Levels}
\label{chap:levitation}
\section{Levitation for Closed QIITs}
\section{Levitation for Infinitary QIITs}

\chapter{Higher Inductive-Inductive Types}
\label{chap:hiit}

\section{Theory of Signatures}
\section{Semantics}

\chapter{Reductions}

\section{Finitary Inductive Types}
\section{Finitary Inductive-Inductive Types}
\section{Closed Quotient Inductive-Inductive Types}

\chapter{Conclusion}

\bibliography{references}
\backmatter
\end{document}
